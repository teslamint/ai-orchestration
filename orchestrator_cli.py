import ast
import difflib
import json
import os
import re
import shutil
import subprocess
import shlex
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import selectors
import time
from typing import Any, Dict, List, Optional

import typer
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table

from agent_prompts import AGENT_PROMPTS

# --- ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ Import ---
# (orchestration_context.py, agent_prompts.py íŒŒì¼ì´ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤)
from orchestration_context import (
    ActionType,
    CodeReviewItem,
    CodeReviewResult,
    ExecutionLog,
    OrchestrationContext,
    ReviewItemType,
    ReviewSeverity,
    Task,
)

app = typer.Typer()
console = Console()
DEBUG = False
DEBUG_LOG_PATH: Path | None = None


def _generate_command_slug(command: str, max_length: int = 30) -> str:
    """Generate a short slug from command for use in log filenames."""
    slug = re.sub(r'[^\w\s-]', '', command.lower())
    slug = re.sub(r'[\s_-]+', '_', slug)
    slug = slug.strip('_')
    if len(slug) > max_length:
        slug = slug[:max_length].rstrip('_')
    return slug if slug else "cmd"


@dataclass
class CommandExecutionLog:
    """Structured log entry for a single command execution attempt."""
    timestamp: str
    command: str
    cwd: Optional[str]
    exit_code: int
    stdout: str
    stderr: str
    duration_ms: int
    attempt: int


@dataclass
class CommandExecutionSummary:
    """Summary of all attempts for a single command execution."""
    command_id: str
    command: str
    cwd: Optional[str]
    started_at: str
    finished_at: str
    total_attempts: int
    final_status: str  # "success", "failed", "skipped"
    final_exit_code: Optional[int]
    attempts: List[Dict[str, Any]]


def _truncate_stderr(stderr: str, max_lines: int = 5, max_chars: int = 200) -> str:
    """Truncate stderr to a short excerpt for summary reports."""
    if not stderr:
        return "(no stderr)"
    lines = stderr.strip().splitlines()
    if len(lines) > max_lines:
        lines = lines[:max_lines]
    truncated = "\n".join(lines)
    if len(truncated) > max_chars:
        truncated = truncated[:max_chars] + "..."
    return truncated


def _print_failure_summary(
    command: str,
    total_attempts: int,
    final_exit_code: Optional[int],
    last_stderr: str,
) -> None:
    """Print a concise summary report for a failed command execution."""
    stderr_excerpt = _truncate_stderr(last_stderr)
    console.print("\n[bold red]â”â”â” Command Execution Failed â”â”â”[/bold red]")
    console.print(f"[bold]Command:[/bold] {command}")
    console.print(f"[bold]Attempts:[/bold] {total_attempts}")
    console.print(f"[bold]Exit Code:[/bold] {final_exit_code}")
    console.print(f"[bold]Stderr:[/bold]\n{stderr_excerpt}")
    console.print("[bold red]â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[/bold red]\n")


@dataclass
class CommandExecutor:
    """Executor for shell commands with auto-approve, retry, and structured logging."""
    auto_approve: bool = False
    retries: int = 1
    log_directory: Path = field(default_factory=lambda: Path("execution_logs"))
    _execution_counter: int = field(default=0, init=False)
    
    def __post_init__(self):
        self.log_directory = Path(self.log_directory)
        self.log_directory.mkdir(parents=True, exist_ok=True)
    
    def _generate_command_id(self, command: str) -> str:
        """Generate a unique command ID with timestamp, counter, and slug."""
        self._execution_counter += 1
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        slug = _generate_command_slug(command)
        return f"{timestamp}_{self._execution_counter:04d}_{slug}"
    
    def _write_execution_log(self, summary: CommandExecutionSummary) -> Path:
        """Write a complete execution log (all attempts + final status) to a JSON file."""
        log_file = self.log_directory / f"{summary.command_id}.json"
        
        log_data = {
            "command_id": summary.command_id,
            "command": summary.command,
            "cwd": summary.cwd,
            "started_at": summary.started_at,
            "finished_at": summary.finished_at,
            "total_attempts": summary.total_attempts,
            "final_status": summary.final_status,
            "final_exit_code": summary.final_exit_code,
            "attempts": summary.attempts,
        }
        
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
        
        return log_file
    
    def run(self, command: str, cwd: Optional[str] = None) -> tuple[bool, str, List[CommandExecutionLog]]:
        """
        Execute a command with optional confirmation, retries, and structured logging.
        
        Returns:
            tuple of (success: bool, output: str, logs: List[CommandExecutionLog])
        """
        command_id = self._generate_command_id(command)
        started_at = datetime.now().isoformat()
        logs: List[CommandExecutionLog] = []
        attempts_data: List[Dict[str, Any]] = []
        
        if not self.auto_approve:
            should_run = typer.confirm(f"Do you want to execute this command: {command}?")
            if not should_run:
                finished_at = datetime.now().isoformat()
                summary = CommandExecutionSummary(
                    command_id=command_id,
                    command=command,
                    cwd=cwd,
                    started_at=started_at,
                    finished_at=finished_at,
                    total_attempts=0,
                    final_status="skipped",
                    final_exit_code=None,
                    attempts=[],
                )
                log_file = self._write_execution_log(summary)
                if DEBUG:
                    console.print(f"[dim]Execution log written to: {log_file}[/dim]")
                return False, "Command execution skipped by user.", []
        
        max_attempts = 1 + self.retries
        final_status = "failed"
        final_exit_code: Optional[int] = None
        final_output = ""
        last_stderr = ""
        
        for attempt in range(max_attempts):
            timestamp = datetime.now().isoformat()
            start_time = time.monotonic()
            
            try:
                command_args = shlex.split(command)
                result = subprocess.run(
                    command_args,
                    capture_output=True,
                    text=True,
                    encoding="utf-8",
                    cwd=cwd,
                )
                
                end_time = time.monotonic()
                duration_ms = int((end_time - start_time) * 1000)
                
                log_entry = CommandExecutionLog(
                    timestamp=timestamp,
                    command=command,
                    cwd=cwd,
                    exit_code=result.returncode,
                    stdout=result.stdout,
                    stderr=result.stderr,
                    duration_ms=duration_ms,
                    attempt=attempt + 1,
                )
                logs.append(log_entry)
                
                attempt_data = {
                    "timestamp": log_entry.timestamp,
                    "attempt": log_entry.attempt,
                    "exit_code": log_entry.exit_code,
                    "stdout": log_entry.stdout,
                    "stderr": log_entry.stderr,
                    "duration_ms": log_entry.duration_ms,
                }
                attempts_data.append(attempt_data)
                
                last_stderr = result.stderr
                
                if result.returncode == 0:
                    final_status = "success"
                    final_exit_code = result.returncode
                    final_output = result.stdout.strip()
                    break
                
                final_exit_code = result.returncode
                final_output = f"Command failed with exit code {result.returncode}: {result.stderr}"
                
                if attempt < max_attempts - 1:
                    console.print(f"[yellow]Command failed (attempt {attempt + 1}/{max_attempts}), retrying...[/yellow]")
                    continue
                
            except Exception as e:
                end_time = time.monotonic()
                duration_ms = int((end_time - start_time) * 1000)
                
                log_entry = CommandExecutionLog(
                    timestamp=timestamp,
                    command=command,
                    cwd=cwd,
                    exit_code=-1,
                    stdout="",
                    stderr=str(e),
                    duration_ms=duration_ms,
                    attempt=attempt + 1,
                )
                logs.append(log_entry)
                
                attempt_data = {
                    "timestamp": log_entry.timestamp,
                    "attempt": log_entry.attempt,
                    "exit_code": log_entry.exit_code,
                    "stdout": log_entry.stdout,
                    "stderr": log_entry.stderr,
                    "duration_ms": log_entry.duration_ms,
                }
                attempts_data.append(attempt_data)
                
                last_stderr = str(e)
                final_exit_code = -1
                final_output = f"Command execution error: {e}"
                
                if attempt < max_attempts - 1:
                    console.print(f"[yellow]Command failed (attempt {attempt + 1}/{max_attempts}), retrying...[/yellow]")
                    continue
        
        finished_at = datetime.now().isoformat()
        
        summary = CommandExecutionSummary(
            command_id=command_id,
            command=command,
            cwd=cwd,
            started_at=started_at,
            finished_at=finished_at,
            total_attempts=len(attempts_data),
            final_status=final_status,
            final_exit_code=final_exit_code,
            attempts=attempts_data,
        )
        log_file = self._write_execution_log(summary)
        
        if DEBUG:
            console.print(f"[dim]Execution log written to: {log_file}[/dim]")
        
        if final_status == "failed":
            _print_failure_summary(
                command=command,
                total_attempts=len(attempts_data),
                final_exit_code=final_exit_code,
                last_stderr=last_stderr,
            )
        
        return final_status == "success", final_output, logs


@dataclass
class OrchestratorConfig:
    """Configuration object for orchestrator settings."""
    auto_approve: bool = False
    auto_run: bool = False
    debug: bool = False
    debug_log_path: Optional[Path] = None
    workspace_path: Path = field(default_factory=lambda: Path("./workspace"))
    command_executor: Optional[CommandExecutor] = None


# Global config instance
_config: Optional[OrchestratorConfig] = None


def get_config() -> OrchestratorConfig:
    """Get the current orchestrator configuration."""
    global _config
    if _config is None:
        _config = OrchestratorConfig()
    return _config


def set_config(config: OrchestratorConfig) -> None:
    """Set the orchestrator configuration."""
    global _config
    _config = config


def get_command_executor() -> CommandExecutor:
    """Get the CommandExecutor from current config, creating one if needed."""
    config = get_config()
    if config.command_executor is None:
        config.command_executor = CommandExecutor(
            auto_approve=config.auto_approve,
            retries=1,
            log_directory=Path("execution_logs"),
        )
    return config.command_executor


# --- Configuration: ëª…ë ¹ì–´ ê²½ë¡œ ì„¤ì • ---
GEMINI_BIN = shutil.which("gemini") or "gemini"
CHATGPT_BIN = shutil.which("codex") or "codex"
CLAUDE_BIN = shutil.which("claude") or "claude"

# --- 1. Helper Functions ---


def _run_shell_command(
    args: list[str],
    cwd: str = None,
    stage: str = "",
    parse_stream_json: bool = False,
) -> str:
    """í„°ë¯¸ë„ ëª…ë ¹ì–´ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
    cmd = args[0]
    if not shutil.which(cmd) and not cmd.startswith("/"):
        raise RuntimeError(
            f"ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{cmd}'. ì„¤ì¹˜ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        )

    try:
        if DEBUG:
            console.print(f"[dim]Running command: {' '.join(args)}[/dim]")
            if cwd:
                console.print(f"[dim]Working directory: {cwd}[/dim]")
            merged_output: list[str] = []
            parsed_chunks: list[str] = []
            # encoding='utf-8'ë¡œ í•œê¸€ ì²˜ë¦¬ ë³´ì¥
            process = subprocess.Popen(
                args,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                encoding="utf-8",
                cwd=cwd,
            )
            assert process.stdout is not None
            sel = selectors.DefaultSelector()
            sel.register(process.stdout, selectors.EVENT_READ)
            last_output = time.monotonic()
            last_heartbeat = last_output
            while sel.get_map():
                events = sel.select(timeout=1.0)
                if not events:
                    now = time.monotonic()
                    if now - last_output >= 10 and now - last_heartbeat >= 10:
                        heartbeat = "[waiting for output]"
                        console.print(
                            f"[dim]{stage} | {heartbeat}[/dim]"
                            if stage
                            else f"[dim]{heartbeat}[/dim]"
                        )
                        _append_debug_log_line(stage or cmd, heartbeat)
                        last_heartbeat = now
                    if process.poll() is not None and not events:
                        break
                    continue
                for key, _ in events:
                    line = key.fileobj.readline()
                    if line == "":
                        sel.unregister(key.fileobj)
                        continue
                    line = line.rstrip("\n")
                    merged_output.append(line)
                    console.print(
                        f"[dim]{stage} | {line}[/dim]"
                        if stage
                        else f"[dim]{line}[/dim]"
                    )
                    _append_debug_log_line(stage or cmd, line)
                    last_output = time.monotonic()
                    if parse_stream_json:
                        parsed_chunks.extend(_extract_stream_json_text(line))
            returncode = process.wait()
            combined = "\n".join(merged_output)
            if DEBUG:
                console.print(
                    f"[dim]Command finished: returncode={returncode}, "
                    f"stdout_len={len(combined)}, stderr_len=0[/dim]"
                )
            if returncode != 0:
                raise RuntimeError(f"ëª…ë ¹ì–´ ì‹¤í–‰ ì‹¤íŒ¨: returncode={returncode}")
            if parse_stream_json:
                if not parsed_chunks:
                    parsed_chunks = _extract_stream_json_from_combined(combined)
                if parsed_chunks:
                    return "".join(parsed_chunks).strip()
                return ""
            return combined.strip()
        else:
            # encoding='utf-8'ë¡œ í•œê¸€ ì²˜ë¦¬ ë³´ì¥
            result = subprocess.run(
                args, capture_output=True, text=True, check=True, encoding="utf-8", cwd=cwd
            )
            return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        console.print(f"[bold red]CLI Error ({cmd}):[/bold red] {e.stderr}")
        raise RuntimeError(f"ëª…ë ¹ì–´ ì‹¤í–‰ ì‹¤íŒ¨: {e.stderr}")


def _write_debug_log(stage: str, content: str) -> None:
    if not DEBUG or DEBUG_LOG_PATH is None:
        return
    try:
        DEBUG_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
        with DEBUG_LOG_PATH.open("a", encoding="utf-8") as f:
            f.write(f"\n\n==== {stage} ====\n")
            f.write(content)
    except Exception as e:
        console.print(f"[yellow]Debug log write failed: {e}[/yellow]")


def _append_debug_log_line(stage: str, line: str) -> None:
    if not DEBUG or DEBUG_LOG_PATH is None:
        return
    try:
        DEBUG_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
        with DEBUG_LOG_PATH.open("a", encoding="utf-8") as f:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"[{timestamp}] {stage}: {line}\n")
    except Exception as e:
        console.print(f"[yellow]Debug log write failed: {e}[/yellow]")


def _normalize_run_command(instruction: str) -> str:
    """Extract a raw command string from a run_command instruction."""
    text = instruction.strip()
    backtick = re.findall(r"`([^`]+)`", text)
    if backtick:
        return backtick[-1].strip()
    if ":" in text:
        after_colon = text.split(":")[-1].strip()
        if after_colon:
            return after_colon
    match = re.findall(r"\buv\s+[^\n]+", text)
    if match:
        return match[-1].strip()
    return text


def _extract_stream_json_text(line: str) -> list[str]:
    try:
        payload = json.loads(line)
    except json.JSONDecodeError:
        return []
    texts: list[str] = []
    if isinstance(payload, dict):
        if isinstance(payload.get("result"), str):
            texts.append(payload["result"])
        delta = payload.get("delta")
        if isinstance(delta, dict):
            text = delta.get("text")
            if isinstance(text, str):
                texts.append(text)
        content_block = payload.get("content_block")
        if isinstance(content_block, dict):
            text = content_block.get("text")
            if isinstance(text, str):
                texts.append(text)
        if isinstance(payload.get("text"), str):
            texts.append(payload["text"])
        content = payload.get("content")
        if isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and isinstance(item.get("text"), str):
                    texts.append(item["text"])
    return texts


def _extract_stream_json_from_combined(combined: str) -> list[str]:
    texts: list[str] = []
    for line in combined.splitlines():
        texts.extend(_extract_stream_json_text(line))
    return texts


def _extract_json_list(text: str) -> List[Dict[str, Any]]:
    """í…ìŠ¤íŠ¸ì—ì„œ JSON List ([...]) ì¶”ì¶œ"""
    try:
        return json.loads(text)
    except:
        pass
    decoder = json.JSONDecoder()
    candidates: list[list[dict[str, Any]]] = []
    idx = 0
    while True:
        idx = text.find("[", idx)
        if idx == -1:
            break
        try:
            parsed, end = decoder.raw_decode(text[idx:])
        except json.JSONDecodeError:
            idx += 1
            continue
        if isinstance(parsed, list) and all(isinstance(item, dict) for item in parsed):
            candidates.append(parsed)
        idx = idx + end
    if candidates:
        return candidates[-1]

    return []


def _extract_code_content(text: str) -> str:
    """Claude ì¶œë ¥ì—ì„œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ë‚´ë¶€ë§Œ ì¶”ì¶œ"""
    start_match = re.search(r"```(?:[\w\+\-\.]+)?\s*\n", text)
    if not start_match:
        return text.strip()
    start_index = start_match.end()
    end_index = text.rfind("\n```")
    if end_index != -1 and end_index > start_index:
        return text[start_index:end_index].strip()
    return text[start_index:].strip()

def _detect_tooling(repo_root: Path) -> str:
    pyproject_path = repo_root / "pyproject.toml"
    poetry_lock = repo_root / "poetry.lock"
    pdm_config = repo_root / "pdm.toml"
    uv_lock = repo_root / "uv.lock"
    tooling = []

    if uv_lock.exists():
        tooling.append("uv")

    if pyproject_path.exists():
        content = pyproject_path.read_text(encoding="utf-8", errors="ignore").lower()
        if "[tool.poetry]" in content:
            tooling.append("poetry")
        if "[tool.pdm]" in content:
            tooling.append("pdm")
        if "[tool.uv]" in content and "uv" not in tooling:
            tooling.append("uv")

    if poetry_lock.exists() and "poetry" not in tooling:
        tooling.append("poetry")
    if pdm_config.exists() and "pdm" not in tooling:
        tooling.append("pdm")

    if not tooling:
        return "unknown"
    if len(tooling) == 1:
        return tooling[0]
    return ", ".join(tooling)


def _generate_diff(old_content: str, new_content: str, file_path: str) -> str:
    """ë‘ ì½”ë“œ ë²„ì „ ê°„ì˜ unified diff ìƒì„±"""
    old_lines = old_content.splitlines(keepends=True)
    new_lines = new_content.splitlines(keepends=True)

    diff = difflib.unified_diff(
        old_lines,
        new_lines,
        fromfile=f"a/{file_path}",
        tofile=f"b/{file_path}",
        lineterm=""
    )
    return "".join(diff)


# --- 2. Agent Runners (ê¸°ì¡´ íŒŒì¼ í™œìš©) ---


def run_gemini_brainstorm(context: OrchestrationContext):
    """Stage 1: Gemini ì‹¤í–‰"""
    # agent_prompts.pyì˜ í…œí”Œë¦¿ ì‚¬ìš©
    tooling_context = _detect_tooling(Path.cwd())
    if DEBUG:
        console.print(f"[dim]Tooling context: {tooling_context}[/dim]")
    prompt = AGENT_PROMPTS["brainstormer"]["user"].format(
        user_goal=context.user_goal,
        tooling_context=tooling_context,
    )
    if DEBUG:
        console.print("[dim]Gemini prompt prepared[/dim]")

    # Gemini CLI ì‹¤í–‰ (Markdown ì¶œë ¥ì„ ìœ„í•´ json í¬ë§· ê°•ì œí•˜ì§€ ì•ŠìŒ, í•„ìš”ì‹œ ì¡°ì •)
    # í…œí”Œë¦¿ì—ì„œ "Markdwon list format"ì„ ìš”êµ¬í•˜ë¯€ë¡œ í…ìŠ¤íŠ¸ë¡œ ë°›ìŠµë‹ˆë‹¤.
    cmd = [GEMINI_BIN, prompt]

    output = _run_shell_command(cmd, stage="GEMINI")
    if DEBUG:
        output_preview = output if len(output) <= 2000 else f"{output[:2000]}...\n[truncated]"
        console.print(
            Panel(
                output_preview,
                title="Gemini raw output (preview)",
                border_style="cyan",
            )
        )
        _write_debug_log("Gemini raw output", output)

    # ê²°ê³¼ ì €ì¥ (OrchestrationContext í•„ë“œì— ë§ê²Œ)
    context.brainstorming_ideas = output


def run_codex_brainstorm_review(context: OrchestrationContext):
    """Stage 2: Codexê°€ Gemini ë¸Œë ˆì¸ìŠ¤í† ë°ì„ ë¦¬ë·°/ì •ë¦¬"""
    tooling_context = _detect_tooling(Path.cwd())
    if DEBUG:
        console.print(f"[dim]Tooling context: {tooling_context}[/dim]")

    prompt = AGENT_PROMPTS["brainstorming_reviewer"]["user"].format(
        user_goal=context.user_goal,
        tooling_context=tooling_context,
        brainstorming_ideas=context.brainstorming_ideas,
    )

    if DEBUG:
        console.print("[dim]Codex brainstorm review prompt prepared[/dim]")

    cmd = [CHATGPT_BIN, "exec", prompt]
    output = _run_shell_command(cmd, stage="CODEX_REVIEW")

    if DEBUG:
        output_preview = output if len(output) <= 2000 else f"{output[:2000]}...\n[truncated]"
        console.print(
            Panel(
                output_preview,
                title="Codex Brainstorm Review (preview)",
                border_style="yellow",
            )
        )
        _write_debug_log("Codex Brainstorm Review output", output)

    # ê²°ê³¼ ì €ì¥
    context.refined_brainstorming = output

    # "Recommended Approach" ì„¹ì…˜ì—ì„œ ì¶”ì²œ ì ‘ê·¼ë²• ì¶”ì¶œ
    recommended_match = re.search(
        r"##\s*Recommended\s+Approach\s*\n+(.*?)(?:\n##|\Z)",
        output,
        re.DOTALL | re.IGNORECASE
    )
    if recommended_match:
        context.brainstorming_review_notes = recommended_match.group(1).strip()


def run_codex_planning(context: OrchestrationContext):
    """Stage 3: Codex ê³„íš ìƒì„±"""
    # agent_prompts.pyì˜ í…œí”Œë¦¿ ì‚¬ìš©
    tooling_context = _detect_tooling(Path.cwd())
    if DEBUG:
        console.print(f"[dim]Tooling context: {tooling_context}[/dim]")

    # refined_brainstormingì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
    brainstorming_to_use = (
        context.refined_brainstorming
        if context.refined_brainstorming
        else context.brainstorming_ideas
    )

    prompt = AGENT_PROMPTS["planner"]["user"].format(
        user_goal=context.user_goal,
        tooling_context=tooling_context,
        brainstorming_ideas=brainstorming_to_use,
        selected_approach=context.selected_approach,
    )
    if DEBUG:
        console.print("[dim]Codex prompt prepared[/dim]")

    cmd = [CHATGPT_BIN, "exec", prompt]
    output = _run_shell_command(cmd, stage="CODEX")
    if DEBUG:
        output_preview = output if len(output) <= 2000 else f"{output[:2000]}...\n[truncated]"
        console.print(
            Panel(
                output_preview,
                title="Codex raw output (preview)",
                border_style="green",
            )
        )
        _write_debug_log("Codex raw output", output)

    # JSON Parsing ë° Pydantic Task ë³€í™˜
    json_plan = _extract_json_list(output)

    tasks = []
    for item in json_plan:
        try:
            # Pydantic ëª¨ë¸(Task)ë¡œ ë³€í™˜
            task = Task(**item)
            tasks.append(task)
        except Exception as e:
            console.print(f"[yellow]Task íŒŒì‹± ì‹¤íŒ¨ (ê±´ë„ˆëœ€): {e}[/yellow]")

    context.implementation_plan = tasks


def run_claude_executor(
    context: OrchestrationContext, task: Task, max_retries: int = 3
):
    """Stage 4: Claude ì‹¤í–‰ (Self-Healing ë° diff ìˆ˜ì§‘ í¬í•¨)"""

    # íŒŒì¼ ì½ê¸° (ìˆ˜ì • ì‘ì—…ì¸ ê²½ìš° ê¸°ì¡´ ì½”ë“œ í•„ìš”)
    existing_code = ""
    target_path = context.workspace_path / task.file_path

    # ê¸°ì¡´ ë‚´ìš© ì½ê¸° (diff ìƒì„±ìš©)
    if target_path.exists():
        try:
            existing_code = target_path.read_text(encoding="utf-8")
        except Exception:
            pass

    # agent_prompts.pyì˜ í…œí”Œë¦¿ ì‚¬ìš©
    prompt = AGENT_PROMPTS["executor"]["user"].format(
        user_goal=context.user_goal,
        step_id=task.step_id,
        action_type=task.action_type.value,
        file_path=str(task.file_path),
        instruction=task.instruction,
        existing_code=existing_code,
    )

    # Self-Healing Loop
    current_prompt = prompt
    for attempt in range(max_retries + 1):
        if DEBUG:
            console.print(
                f"[dim]Claude attempt {attempt + 1}/{max_retries + 1} "
                f"for {task.action_type.value} {task.file_path}[/dim]"
            )
        # 1. ì‹¤í–‰
        cmd = [
            CLAUDE_BIN,
            current_prompt,
            "--print",
            "--tools",
            "",
            "--disable-slash-commands",
            "--permission-mode",
            "dontAsk",
        ]
        if DEBUG:
            cmd.extend(["--output-format", "stream-json"])
        if DEBUG:
            console.print(f"[dim]Claude command: {cmd[0]} <prompt> --print[/dim]")
        raw_output = _run_shell_command(
            cmd,
            stage=f"CLAUDE task {task.step_id}",
            parse_stream_json=DEBUG,
        )
        if DEBUG:
            output_preview = raw_output if len(raw_output) <= 2000 else f"{raw_output[:2000]}...\n[truncated]"
            console.print(
                Panel(
                    output_preview,
                    title="Claude raw output (preview)",
                    border_style="magenta",
                )
            )
            _write_debug_log(
                f"Claude raw output (task {task.step_id}, attempt {attempt + 1})",
                raw_output,
            )
        code_content = _extract_code_content(raw_output)
        if code_content.lstrip().startswith("{") and "\"type\"" in code_content:
            if DEBUG:
                console.print(
                    "[yellow]Claude output appears to be stream JSON; skipping file write.[/yellow]"
                )
            context.execution_logs.append(
                ExecutionLog(
                    step_id=task.step_id,
                    success=False,
                    message="Claude output looked like stream JSON; skipped write.",
                )
            )
            return
        if DEBUG and not code_content.strip():
            console.print(
                "[yellow]Claude output empty after parsing; skipping file write.[/yellow]"
            )
            context.execution_logs.append(
                ExecutionLog(
                    step_id=task.step_id,
                    success=False,
                    message="Claude output empty after parsing; skipped write.",
                )
            )
            return
        if DEBUG:
            console.print(
                f"[dim]Claude output length: {len(raw_output)} "
                f"chars, extracted length: {len(code_content)} chars[/dim]"
            )

        # 2. ë¬¸ë²• ê²€ì‚¬ (Python íŒŒì¼ì¸ ê²½ìš°ë§Œ)
        if str(task.file_path).endswith(".py") and code_content:
            try:
                ast.parse(code_content)
            except SyntaxError as e:
                console.print(
                    f"[bold yellow]âš ï¸ Syntax Error (Attempt {attempt + 1}):[/bold yellow] {e.msg}"
                )
                if attempt < max_retries:
                    current_prompt = (
                        f"The code has a SyntaxError: {e.msg} at line {e.lineno}.\n"
                        f"Your code:\n```python\n{code_content}\n```\n"
                        "Please FIX it and return ONLY the corrected code."
                    )
                    continue
                else:
                    console.print("[bold red]âŒ ë¬¸ë²• ìˆ˜ì • ì‹¤íŒ¨[/bold red]")

        # 3. íŒŒì¼ ì €ì¥ (ì„±ê³µ ì‹œ loop íƒˆì¶œ)
        if task.action_type in [ActionType.CREATE_FILE, ActionType.EDIT_FILE]:
            try:
                # workspace_path ê¸°ì¤€ ì ˆëŒ€ ê²½ë¡œ ìƒì„± (contextì˜ validatorê°€ ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë‚˜ ì•ˆì „ì¥ì¹˜)
                full_path = context.workspace_path / task.file_path
                full_path.parent.mkdir(parents=True, exist_ok=True)

                with open(full_path, "w", encoding="utf-8") as f:
                    f.write(code_content)

                # diff ìƒì„± ë° ì €ì¥
                diff = _generate_diff(existing_code, code_content, str(task.file_path))
                context.generated_diffs[str(task.file_path)] = diff

                # ë¡œê·¸ ê¸°ë¡
                log = ExecutionLog(
                    step_id=task.step_id,
                    success=True,
                    message=f"íŒŒì¼ ì‘ì„± ì™„ë£Œ: {task.file_path}",
                )
                context.execution_logs.append(log)
                console.print(f"[bold green]Saved:[/bold green] {full_path}")
                return  # ì„±ê³µ ì¢…ë£Œ
            except Exception as e:
                console.print(f"[red]íŒŒì¼ ì“°ê¸° ì˜¤ë¥˜: {e}[/red]")
                return

    # ì‹¤íŒ¨ ë¡œê·¸
    context.execution_logs.append(
        ExecutionLog(
            step_id=task.step_id, success=False, message="ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ ë˜ëŠ” ì˜¤ë¥˜"
        )
    )


def execute_run_command(
    task: Task,
    context: OrchestrationContext,
) -> None:
    """Execute a RUN_COMMAND task using the global CommandExecutor."""
    console.print(f"[bold yellow]Command to run:[/bold yellow] {task.instruction}")
    if DEBUG:
        console.print(f"[dim]RUN_COMMAND task id: {task.step_id}[/dim]")
    
    executor = get_command_executor()
    current_config = get_config()
    command_text = _normalize_run_command(task.instruction)
    
    if current_config.auto_run and not executor.auto_approve:
        executor.auto_approve = True
    
    success, output, exec_logs = executor.run(
        command_text,
        cwd=str(context.workspace_path),
    )
    
    if success:
        context.execution_logs.append(
            ExecutionLog(
                step_id=task.step_id,
                success=True,
                message=f"Command executed: {task.instruction}",
                output=output,
            )
        )
        console.print(f"[green]âœ” Command executed successfully.[/green]")
        console.print(f"[dim]{output}[/dim]")
    else:
        context.execution_logs.append(
            ExecutionLog(
                step_id=task.step_id,
                success=False,
                message=f"Command failed: {output}",
                output=output,
            )
        )
        if "skipped by user" in output.lower():
            console.print("[yellow]Skipped command execution by user.[/yellow]")
        else:
            console.print(f"[bold red]âŒ Command execution failed: {output}[/bold red]")


def run_codex_code_review(context: OrchestrationContext):
    """Stage 5: Codex ì½”ë“œ ë¦¬ë·°"""

    # ì‹¤í–‰ëœ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    file_list = []
    file_contents = {}
    for log in context.execution_logs:
        if log.success:
            # Taskì—ì„œ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            for task in context.implementation_plan:
                if task.step_id == log.step_id:
                    if task.action_type in [ActionType.CREATE_FILE, ActionType.EDIT_FILE]:
                        file_path = str(task.file_path)
                        file_list.append(file_path)
                        # íŒŒì¼ ë‚´ìš© ì½ê¸°
                        full_path = context.workspace_path / task.file_path
                        if full_path.exists():
                            try:
                                file_contents[file_path] = full_path.read_text(encoding="utf-8")
                            except Exception:
                                file_contents[file_path] = "[Error reading file]"

    # ì‹¤í–‰ ìš”ì•½ ìƒì„±
    success_count = sum(1 for log in context.execution_logs if log.success)
    fail_count = sum(1 for log in context.execution_logs if not log.success)
    execution_summary = f"Total tasks: {len(context.execution_logs)}, Success: {success_count}, Failed: {fail_count}"

    # ê³„íš ìš”ì•½ ìƒì„±
    plan_summary = "\n".join([
        f"- Step {t.step_id}: {t.action_type.value} {t.file_path}"
        for t in context.implementation_plan
    ])

    # Diff ë¬¸ìì—´ ìƒì„±
    code_diffs = "\n\n".join([
        f"=== {path} ===\n{diff}"
        for path, diff in context.generated_diffs.items()
    ]) if context.generated_diffs else "(No diffs available)"

    # íŒŒì¼ ë‚´ìš© ë¬¸ìì—´ ìƒì„±
    file_contents_str = "\n\n".join([
        f"=== {path} ===\n```\n{content}\n```"
        for path, content in file_contents.items()
    ]) if file_contents else "(No files)"

    prompt = AGENT_PROMPTS["code_reviewer"]["user"].format(
        user_goal=context.user_goal,
        plan_summary=plan_summary,
        file_list="\n".join([f"- {f}" for f in file_list]),
        execution_summary=execution_summary,
        code_diffs=code_diffs,
        file_contents=file_contents_str,
    )

    if DEBUG:
        console.print("[dim]Codex code review prompt prepared[/dim]")

    cmd = [CHATGPT_BIN, "exec", prompt]
    output = _run_shell_command(cmd, stage="CODEX_CODE_REVIEW")

    if DEBUG:
        output_preview = output if len(output) <= 2000 else f"{output[:2000]}...\n[truncated]"
        console.print(
            Panel(
                output_preview,
                title="Codex Code Review (preview)",
                border_style="red",
            )
        )
        _write_debug_log("Codex Code Review output", output)

    # JSON íŒŒì‹±
    try:
        # JSON ì¶”ì¶œ ì‹œë„
        json_match = re.search(r'\{[\s\S]*\}', output)
        if json_match:
            review_data = json.loads(json_match.group())

            # CodeReviewResult ìƒì„±
            items = []
            for item_data in review_data.get("items", []):
                try:
                    items.append(CodeReviewItem(
                        item_id=item_data["item_id"],
                        file_path=Path(item_data["file_path"]),
                        line_start=item_data.get("line_start"),
                        line_end=item_data.get("line_end"),
                        review_type=ReviewItemType(item_data["review_type"]),
                        severity=ReviewSeverity(item_data["severity"]),
                        description=item_data["description"],
                        suggestion=item_data["suggestion"],
                        code_snippet=item_data.get("code_snippet"),
                    ))
                except Exception as e:
                    if DEBUG:
                        console.print(f"[yellow]Review item parsing failed: {e}[/yellow]")

            context.code_review_result = CodeReviewResult(
                reviewed_at=review_data.get("reviewed_at", datetime.now().isoformat()),
                total_files_reviewed=review_data.get("total_files_reviewed", len(file_list)),
                items=items,
                overall_assessment=review_data.get("overall_assessment", ""),
                requires_fixes=review_data.get("requires_fixes", len(items) > 0),
            )
    except json.JSONDecodeError as e:
        console.print(f"[red]Code review JSON parsing failed: {e}[/red]")
        # ë¹ˆ ë¦¬ë·° ê²°ê³¼ ìƒì„±
        context.code_review_result = CodeReviewResult(
            reviewed_at=datetime.now().isoformat(),
            total_files_reviewed=len(file_list),
            items=[],
            overall_assessment="Review parsing failed",
            requires_fixes=False,
        )


def _prompt_fix_selection(items: List[CodeReviewItem]) -> List[CodeReviewItem]:
    """ì‚¬ìš©ìì—ê²Œ ìˆ˜ì •í•  ë¦¬ë·° í•­ëª©ì„ ì„ íƒí•˜ë„ë¡ ìš”ì²­"""
    if not items:
        return []

    # í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ë¦¬ë·° í•­ëª© í‘œì‹œ
    table = Table(title="Code Review Items")
    table.add_column("#", style="cyan", width=4)
    table.add_column("Severity", style="bold")
    table.add_column("Type", style="dim")
    table.add_column("File", style="green")
    table.add_column("Description")

    severity_colors = {
        "critical": "bold red",
        "high": "red",
        "medium": "yellow",
        "low": "blue",
        "info": "dim",
    }

    for i, item in enumerate(items, 1):
        color = severity_colors.get(item.severity.value, "white")
        table.add_row(
            str(i),
            f"[{color}]{item.severity.value.upper()}[/{color}]",
            item.review_type.value,
            str(item.file_path),
            item.description[:60] + "..." if len(item.description) > 60 else item.description,
        )

    console.print(table)
    console.print("\nOptions:")
    console.print("  [bold]a[/bold] - Apply all fixes")
    console.print("  [bold]n[/bold] - Skip all fixes")
    console.print("  [bold]1,2,3[/bold] - Select specific items (comma-separated)")
    console.print("  [bold]c[/bold] - Critical and High only")

    choice = typer.prompt("Enter your choice", default="a")

    if choice.lower() == "a":
        return items
    elif choice.lower() == "n":
        return []
    elif choice.lower() == "c":
        return [item for item in items if item.severity in [ReviewSeverity.CRITICAL, ReviewSeverity.HIGH]]
    else:
        # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ìˆ«ì íŒŒì‹±
        try:
            indices = [int(x.strip()) - 1 for x in choice.split(",")]
            return [items[i] for i in indices if 0 <= i < len(items)]
        except (ValueError, IndexError):
            console.print("[yellow]Invalid selection. Skipping fixes.[/yellow]")
            return []


def run_claude_fixer(context: OrchestrationContext, review_item: CodeReviewItem, max_retries: int = 2):
    """Stage 6: Claudeê°€ ë¦¬ë·° í”¼ë“œë°± ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œ ìˆ˜ì •"""

    # í˜„ì¬ íŒŒì¼ ë‚´ìš© ì½ê¸°
    target_path = context.workspace_path / review_item.file_path
    current_code = ""
    if target_path.exists():
        try:
            current_code = target_path.read_text(encoding="utf-8")
        except Exception:
            pass

    # ë¼ì¸ ë²”ìœ„ ë¬¸ìì—´ ìƒì„±
    line_range = "N/A"
    if review_item.line_start:
        if review_item.line_end and review_item.line_end != review_item.line_start:
            line_range = f"{review_item.line_start}-{review_item.line_end}"
        else:
            line_range = str(review_item.line_start)

    prompt = AGENT_PROMPTS["fixer"]["user"].format(
        user_goal=context.user_goal,
        file_path=str(review_item.file_path),
        current_code=current_code,
        review_type=review_item.review_type.value,
        severity=review_item.severity.value,
        description=review_item.description,
        suggestion=review_item.suggestion,
        line_range=line_range,
        code_snippet=review_item.code_snippet or "(no snippet)",
    )

    current_prompt = prompt
    for attempt in range(max_retries + 1):
        if DEBUG:
            console.print(
                f"[dim]Claude fix attempt {attempt + 1}/{max_retries + 1} "
                f"for review item {review_item.item_id}[/dim]"
            )

        cmd = [
            CLAUDE_BIN,
            current_prompt,
            "--print",
            "--tools", "",
            "--disable-slash-commands",
            "--permission-mode", "dontAsk",
        ]
        if DEBUG:
            cmd.extend(["--output-format", "stream-json"])

        raw_output = _run_shell_command(
            cmd,
            stage=f"CLAUDE fix item {review_item.item_id}",
            parse_stream_json=DEBUG,
        )

        if DEBUG:
            _write_debug_log(
                f"Claude fix output (item {review_item.item_id}, attempt {attempt + 1})",
                raw_output,
            )

        code_content = _extract_code_content(raw_output)

        # ë¬¸ë²• ê²€ì‚¬ (Python íŒŒì¼ì¸ ê²½ìš°)
        if str(review_item.file_path).endswith(".py") and code_content:
            try:
                ast.parse(code_content)
            except SyntaxError as e:
                console.print(
                    f"[bold yellow]Syntax Error in fix (Attempt {attempt + 1}):[/bold yellow] {e.msg}"
                )
                if attempt < max_retries:
                    current_prompt = (
                        f"The fixed code has a SyntaxError: {e.msg} at line {e.lineno}.\n"
                        f"Your code:\n```python\n{code_content}\n```\n"
                        "Please FIX it and return ONLY the corrected code."
                    )
                    continue
                else:
                    console.print("[bold red]Fix syntax correction failed[/bold red]")

        # íŒŒì¼ ì €ì¥
        try:
            target_path.parent.mkdir(parents=True, exist_ok=True)
            with open(target_path, "w", encoding="utf-8") as f:
                f.write(code_content)

            log = ExecutionLog(
                step_id=review_item.item_id,
                success=True,
                message=f"Fix applied: {review_item.file_path} (item {review_item.item_id})",
            )
            context.fix_execution_logs.append(log)
            console.print(f"[bold green]Fixed:[/bold green] {target_path}")
            return
        except Exception as e:
            console.print(f"[red]File write error during fix: {e}[/red]")
            return

    # ì‹¤íŒ¨ ë¡œê·¸
    context.fix_execution_logs.append(
        ExecutionLog(
            step_id=review_item.item_id,
            success=False,
            message=f"Fix failed for review item {review_item.item_id}"
        )
    )


# --- 3. Main Workflow ---


@app.command()
def main(
    request: str = typer.Argument(..., help="í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­"),
    workspace: str = typer.Option("./workspace", help="ì‘ì—… íŒŒì¼ì´ ìƒì„±ë  í´ë” ê²½ë¡œ"),
    debug: bool = typer.Option(
        False, "--debug", help="ìƒì„¸ ì§„í–‰ ë¡œê·¸ ì¶œë ¥"
    ),
    debug_log: str = typer.Option(
        "./orchestrator_debug_logs",
        "--debug-log",
        help="ë””ë²„ê·¸ ì „ì²´ ì¶œë ¥ ë¡œê·¸ ê²½ë¡œ(ë””ë ‰í„°ë¦¬ ë˜ëŠ” íŒŒì¼).",
    ),
    auto_run: bool = typer.Option(
        False, "--auto-run", help="run_command ë‹¨ê³„ ìë™ ì‹¤í–‰"
    ),
    auto_approve: bool = typer.Option(
        False, "--auto-approve", help="run_command í™•ì¸ì„ ì „ì—­ìœ¼ë¡œ ìë™ ìŠ¹ì¸"
    ),
    skip_review: bool = typer.Option(
        False, "--skip-review", help="ì½”ë“œ ë¦¬ë·° ë‹¨ê³„(Stage 5-6) ê±´ë„ˆë›°ê¸°"
    ),
    max_fix_iterations: int = typer.Option(
        1, "--max-fix-iterations", help="ìµœëŒ€ ë¦¬ë·°-ìˆ˜ì • ë°˜ë³µ íšŸìˆ˜"
    ),
    auto_fix: bool = typer.Option(
        False, "--auto-fix", help="ë¦¬ë·° í•­ëª© ìë™ ìˆ˜ì • (í™•ì¸ ì—†ì´)"
    ),
):
    """
    AI Orchestration Tool (6-Stage):
    Gemini -> Codex Review -> Codex Plan -> Claude Exec -> Codex Review -> Claude Fix
    """
    console.print(
        Panel.fit(
            f"[bold blue]Goal:[/bold blue] {request}", title="ğŸš€ Orchestrator Started"
        )
    )

    global DEBUG, DEBUG_LOG_PATH
    DEBUG = debug
    if debug:
        log_path = Path(debug_log)
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        if log_path.suffix:
            DEBUG_LOG_PATH = log_path.with_name(
                f"{log_path.stem}-{timestamp}{log_path.suffix}"
            ).resolve()
        else:
            DEBUG_LOG_PATH = (log_path / f"orchestrator_debug-{timestamp}.log").resolve()
        if DEBUG:
            console.print(f"[dim]Debug log file: {DEBUG_LOG_PATH}[/dim]")
    else:
        DEBUG_LOG_PATH = None

    command_executor = CommandExecutor(
        auto_approve=auto_approve,
        retries=1,
        log_directory=Path("execution_logs"),
    )

    # Initialize configuration object
    config = OrchestratorConfig(
        auto_approve=auto_approve,
        auto_run=auto_run,
        debug=debug,
        debug_log_path=DEBUG_LOG_PATH,
        workspace_path=Path(workspace),
        command_executor=command_executor,
    )
    set_config(config)

    if DEBUG:
        console.print(f"[dim]Auto-approve mode: {auto_approve}[/dim]")
        console.print(f"[dim]Auto-run mode: {auto_run}[/dim]")

    # 1. ì´ˆê¸° Context ìƒì„±
    context = OrchestrationContext(
        project_name="AI_Project", user_goal=request, workspace_path=Path(workspace)
    )

    # ===== Stage 1: Gemini (Brainstorming) =====
    console.print("\n[bold cyan]Stage 1: Gemini Brainstorming[/bold cyan]")
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        transient=True,
    ) as progress:
        progress.add_task(description="[cyan]Gemini thinking...[/cyan]", total=None)
        run_gemini_brainstorm(context)

    console.print(
        Panel(
            str(context.brainstorming_ideas),
            title="Stage 1: Gemini Ideas",
            border_style="cyan",
        )
    )

    # ===== Stage 2: Codex (Brainstorming Review) =====
    console.print("\n[bold yellow]Stage 2: Codex Brainstorming Review[/bold yellow]")
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        transient=True,
    ) as progress:
        progress.add_task(description="[yellow]Codex reviewing brainstorming...[/yellow]", total=None)
        run_codex_brainstorm_review(context)

    console.print(
        Panel(
            str(context.refined_brainstorming) if context.refined_brainstorming else "(No refined output)",
            title="Stage 2: Refined Brainstorming",
            border_style="yellow",
        )
    )

    # ì‚¬ìš©ìì—ê²Œ ì ‘ê·¼ ë°©ì‹ ì„ íƒ ìš”ì²­ (refined_brainstorming ê¸°ë°˜)
    ideas_to_use = context.refined_brainstorming if context.refined_brainstorming else context.brainstorming_ideas
    option_pattern = re.compile(
        r"^###?\s*(approach|option|plan|ì ‘ê·¼\s*ë°©ì‹)\b", re.IGNORECASE
    )
    lines = [line.strip() for line in ideas_to_use.split("\n")]
    options = [line for line in lines if option_pattern.match(line)]
    if not options:
        # ëŒ€ì•ˆ: "### Approach" íŒ¨í„´ ì‹œë„
        options = [line for line in lines if line.startswith("### ")]
    if not options:
        options = [line for line in lines if line.startswith("- **")]
    if DEBUG:
        console.print(f"[dim]Approach options detected: {len(options)}[/dim]")

    console.print("\n[bold]Please select an approach:[/bold]")
    for i, opt in enumerate(options):
        console.print(f"  {i + 1}: {opt}")
    console.print(f"  {len(options) + 1}: [dim]Custom (enter your own)[/dim]")

    default_choice = 1
    user_goal_lower = context.user_goal.lower()
    if "uv add --dev pytest" in user_goal_lower:
        for idx, opt in enumerate(options, start=1):
            opt_lower = opt.lower()
            if "uv add --dev pytest" in opt_lower or "uv add --dev" in opt_lower:
                default_choice = idx
                break

    choice = typer.prompt(
        "Enter the number of your choice",
        type=int,
        default=default_choice,
    )
    if DEBUG:
        console.print(f"[dim]User selected option: {choice}[/dim]")

    if 1 <= choice <= len(options):
        context.selected_approach = options[choice - 1]
    elif choice == len(options) + 1:
        # ì‚¬ìš©ì ì§ì ‘ ì…ë ¥
        custom_approach = typer.prompt("Enter your custom approach")
        context.selected_approach = custom_approach
    else:
        console.print("[yellow]Invalid selection. Using the first approach as default.[/yellow]")
        context.selected_approach = options[0] if options else ideas_to_use

    console.print(f"[bold green]Selected Approach:[/bold green] {context.selected_approach}")

    # ===== Stage 3: Codex (Planning) =====
    console.print("\n[bold green]Stage 3: Codex Planning[/bold green]")
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        transient=True,
    ) as progress:
        progress.add_task(description="[green]Codex planning...[/green]", total=None)
        run_codex_planning(context)

    if not context.implementation_plan:
        console.print("[red]ê³„íš ìƒì„± ì‹¤íŒ¨[/red]")
        return

    # Pydantic ëª¨ë¸ì„ JSON ì˜ˆì˜ê²Œ ì¶œë ¥
    plan_json = json.dumps(
        [t.model_dump(mode="json") for t in context.implementation_plan],
        indent=2,
        ensure_ascii=False,
    )
    console.print(
        Panel(plan_json, title="Stage 3: Implementation Plan", border_style="green")
    )

    # ===== Stage 4: Claude (Implementation) =====
    console.print("\n[bold magenta]Stage 4: Claude Implementation[/bold magenta]")
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        transient=True,
    ) as progress:
        task_count = len(context.implementation_plan)
        task_id = progress.add_task(
            description="[magenta]Claude coding...[/magenta]", total=task_count
        )

        for task in context.implementation_plan:
            if task.action_type in [ActionType.CREATE_FILE, ActionType.EDIT_FILE]:
                progress.update(task_id, description=f"Writing {task.file_path}...")
                if DEBUG:
                    console.print(
                        f"[dim]Executing file task: {task.action_type.value} "
                        f"{task.file_path}[/dim]"
                    )
                run_claude_executor(context, task)

            elif task.action_type == ActionType.RUN_COMMAND:
                execute_run_command(task, context)

            progress.advance(task_id)

    console.print(
        Panel.fit("[bold]Stage 4 Complete[/bold]", title="Implementation Done", border_style="magenta")
    )

    # ===== Stage 5-6: Code Review and Fixes =====
    if not skip_review:
        fix_iteration = 0
        while fix_iteration < max_fix_iterations:
            fix_iteration += 1
            context.fix_iteration_count = fix_iteration

            # ===== Stage 5: Codex (Code Review) =====
            console.print(f"\n[bold red]Stage 5: Codex Code Review (iteration {fix_iteration}/{max_fix_iterations})[/bold red]")
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                transient=True,
            ) as progress:
                progress.add_task(description="[red]Codex reviewing code...[/red]", total=None)
                run_codex_code_review(context)

            if context.code_review_result:
                review_summary = f"""Overall: {context.code_review_result.overall_assessment}
Files Reviewed: {context.code_review_result.total_files_reviewed}
Issues Found: {len(context.code_review_result.items)}
Requires Fixes: {context.code_review_result.requires_fixes}"""
                console.print(Panel(review_summary, title="Stage 5: Code Review Summary", border_style="red"))

                if context.code_review_result.items:
                    for item in context.code_review_result.items:
                        severity_color = {
                            "critical": "bold red",
                            "high": "red",
                            "medium": "yellow",
                            "low": "blue",
                            "info": "dim",
                        }.get(item.severity.value, "white")
                        console.print(
                            f"  [{severity_color}][{item.severity.value.upper()}][/{severity_color}] "
                            f"{item.review_type.value}: {item.file_path} - {item.description[:60]}..."
                        )

                # ===== Stage 6: Claude (Fixes) =====
                if context.code_review_result.requires_fixes and context.code_review_result.items:
                    console.print(f"\n[bold blue]Stage 6: Claude Fixes (iteration {fix_iteration}/{max_fix_iterations})[/bold blue]")

                    # ìˆ˜ì •í•  í•­ëª© ì„ íƒ
                    if auto_fix:
                        items_to_fix = context.code_review_result.items
                    else:
                        items_to_fix = _prompt_fix_selection(context.code_review_result.items)

                    if items_to_fix:
                        # ì‹¬ê°ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (critical > high > medium > low > info)
                        severity_order = ["critical", "high", "medium", "low", "info"]
                        sorted_items = sorted(
                            items_to_fix,
                            key=lambda x: severity_order.index(x.severity.value)
                        )

                        with Progress(
                            SpinnerColumn(),
                            TextColumn("[progress.description]{task.description}"),
                            transient=True,
                        ) as progress:
                            fix_count = len(sorted_items)
                            fix_task_id = progress.add_task(
                                description="[blue]Claude fixing...[/blue]",
                                total=fix_count
                            )

                            for review_item in sorted_items:
                                progress.update(
                                    fix_task_id,
                                    description=f"Fixing {review_item.file_path} ({review_item.review_type.value})..."
                                )
                                run_claude_fixer(context, review_item)
                                progress.advance(fix_task_id)

                        console.print(
                            Panel.fit(
                                f"[bold]Stage 6 Complete - {len(sorted_items)} fixes applied[/bold]",
                                title="Fixes Applied",
                                border_style="blue"
                            )
                        )

                        # ë°˜ë³µ ë¦¬ë·°ê°€ í•„ìš”í•œ ê²½ìš° ê³„ì†
                        if fix_iteration < max_fix_iterations:
                            continue
                    else:
                        console.print("[dim]No fixes selected. Skipping Stage 6.[/dim]")
                        break
                else:
                    console.print("[green]No fixes required![/green]")
                    break
            else:
                console.print("[yellow]Code review did not produce results.[/yellow]")
                break
    else:
        console.print("[dim]Code review skipped (--skip-review)[/dim]")

    console.print(
        Panel.fit("[bold green]All Done![/bold green]", title="6-Stage Workflow Finished")
    )


if __name__ == "__main__":
    app()